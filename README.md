# Neural-network-in-C
This is my extension for the C group project in Imperial College First year Computing.
## ğŸ“  File structure
```
â”œâ”€â”€ README.md                           // Readme
â”œâ”€â”€ code
â”‚   â”œâ”€â”€ Makefile                        // For easier compilation, type "make mnist" in cmd                    
â”‚   â”œâ”€â”€ adam.c                          // code for Adam optimizer
â”‚   â”œâ”€â”€ adam.h                          // header file for Adam optimizer                                                                            
â”‚   â”œâ”€â”€ ann.c                           // code for neural network (create, predict, train)
â”‚   â”œâ”€â”€ ann.h                           // header file for ann.c
â”‚   â”œâ”€â”€ cnnlayer.c                      // code for Convolutional layers
â”‚   â”œâ”€â”€ cnnlayer.h                      // header file for cnnlayer.c
â”‚   â”œâ”€â”€ conv.c                          // operations for convolutional layers
â”‚   â”œâ”€â”€ data.temp                       // temp file to store training data for graph plotting
â”‚   â”œâ”€â”€ flattenlayer.c                  // flatten layer for CNN
â”‚   â”œâ”€â”€ flattenlayer.h                  // header file for flatten layer
â”‚   â”œâ”€â”€ helper.c                        // code for graph plotting
â”‚   â”œâ”€â”€ helper.h                        // header file for helper.c
â”‚   â”œâ”€â”€ layer.c                         // code for fully connected layers
â”‚   â”œâ”€â”€ layer.h                         // header file for layer.c
â”‚   â”œâ”€â”€ math_funcs.c                    // math functions, e.g. activation and loss functions
â”‚   â”œâ”€â”€ math_funcs.h                    // header file for math_funcs.c
â”‚   â”œâ”€â”€ math_r4t.c                      // operations on the r4t struct 
â”‚   â”œâ”€â”€ math_r4t.h                      // header file for math_r4t.c
â”‚   â”œâ”€â”€ math_structs.c                  // matrix operations
â”‚   â”œâ”€â”€ math_structs.h                  // header file for math_structs.c
â”‚   â”œâ”€â”€ mnist.c                         // Main program -- tests the DNN on the MNIST dataset
â”‚   â”œâ”€â”€ tensor.c                        // implementation of Pytorch Tensors
â”‚   â”œâ”€â”€ tensor.h                        // header file for tensor.c
â”‚   â”œâ”€â”€ tensor_math.c                   // math operations on tensors
â”‚   â””â”€â”€ xor.c                           // testing program to test sigmoidal MLP to learn the XOR function. Use "make xor" to build.
â”œâ”€â”€ data                                // Image data downloaded from official MNIST website
â”‚   â”œâ”€â”€ test-images.idx3-ubyte          
â”‚   â”œâ”€â”€ test-labels.idx1-ubyte
â”‚   â”œâ”€â”€ train-images.idx3-ubyte
â”‚   â””â”€â”€ train-labels.idx1-ubyte
â””â”€â”€ results
    â”œâ”€â”€ mnist_accuracy_100x100.png      // Accuracy graph for 2 hidden layers (100+100), both dropout 0.4, mini-batch GD
    â”œâ”€â”€ mnist_accuracy_100x100_adam.png // Accuracy graph for 2 hidden layers (100+100), only final hidden layer dropout 0.4, Adam optimizer (BEST)
    â”œâ”€â”€ mnist_accuracy_30x30.png        // Accuracy graph for 2 hidden layers (30+30), no dropout, mini-batch GD (INITIAL)
    â”œâ”€â”€ mnist_accuracy_60x60.png        // Accuracy graph for 2 hidden layers (60+60), both dropout 0.4, mini-batch GD
    â”œâ”€â”€ mnist_loss_100x100.png          // Same descriptions as above, but for loss graph (Mean Cross Entropy loss of the whole training set in each epoch)
    â”œâ”€â”€ mnist_loss_100x100_adam.png
    â”œâ”€â”€ mnist_loss_30x30.png
    â”œâ”€â”€ mnist_loss_60x60.png
    â””â”€â”€ xor.png                         // Loss graph (MSE) for XOR network (1 hidden layer, 2 neurons)
```
## ğŸ”¢ Math and Logic
Only the Deep Neural Network (with Fully Connected Layers) is tested with MNIST. Convolutional network is partially complete.
### Network architecture
### Backpropagation
### Mini-batch Gradient Descent
### Dropout 
### Adam Optimizer
### Training and Graph plotting
First load the byte input files of the MNIST images, then split them into batches of 16. Then trains the network for 20 epochs. \
Finally plots the graph using `gnuplot`.
## ğŸ”¨ Build and Run
Prerequisites:
- Linux / WSL (does not work on Windows ğŸ¤·)
- gnuplot (install via `sudo apt install gnuplot` in command line) \
If you encounter 404 Not Found errors while installing `gnuplot`, run `sudo apt-get update` then `sudo apt install gnuplot --fix-missing`. \
Once you have the prerequisites, type `cd code` followed by `make mnist` to generate the object files and `mnist` executable. Then run the program by typing `./mnist`. \
The output images will be produced in the `code` directory.

## ğŸ“Š Results
The validation set is not included in training and is a true reflection of how well the model is doing. \
Here's the results of some different network architectures that I tried:
1. 2 hidden layers (100+100), only final hidden layer dropout 0.4, Adam optimizer (BEST) \
Best network, highest validation accuracy is 97.40%.
2. 2 hidden layers (30+30), no dropout, mini-batch GD (INITIAL) \
Surprisingly the 2nd best network, the best amongst the mini-batch GD networks. Reached highest validation accuracy of 95.59%
3. 2 hidden layers (60+60), both dropout 0.4, mini-batch GD \
Highest validation accuracy 95.10%
5. 2 hidden layers (100+100), both dropout 0.4, mini-batch GD \
Surprisingly worse than 60+60, validation accuracy was only about 94+% \
<br>
From the graphs of 3 and 4, the validation accuracy/loss is consistently better than the training accuracy/loss, which indicates underfitting.

## ğŸŒŸ Credit/Acknowledgment
Credits to my group members Jeffrey Chang and Sam Shariatmadari for writing the code for the convolutional layers.

## ğŸš€ Future Improvements 
 - Put more neurons in the hidden layers. I did not do that because my potato machine will probably explode.
 - Try to use GPU for matrix operations as it provides a significant speedup.
 - (Specific for image classification) Perform some data augmentation to generate more training examples, such as rotating and resizing the images.
